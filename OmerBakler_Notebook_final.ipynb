{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a_rTDU1QhKXt",
        "phxw8TdBKLFN",
        "spjstl9CNEgO",
        "wJmTIAafmRMf",
        "IwEXWB_bn71X",
        "fv81Zu_hrvw7",
        "sLKQuQpKty_L",
        "UQZleeYAt2ZM",
        "38ENW6Jcv5VR",
        "dhkqqK-vvsZo",
        "nqCHJqrTAvle",
        "ea6nQyMAA1dN",
        "-vrDqm6VC1tk",
        "Rq1A38XhE4zT",
        "dn9_qFgwFYhQ",
        "bDAaqElDGDfy",
        "Xk3aBepsGmHK",
        "NIdGwoyoHbU3",
        "Gd-4FgRoIlOC",
        "Xw9va3sKI4XA",
        "u97GhUA_JH04",
        "EzVnI62EJZ9X",
        "MvnuUU-ESdft",
        "Pqq2fkQgTmX-",
        "S6fTvxXpVPHp",
        "i9OGDcKSXWy0",
        "RQLnzY8KuL-R",
        "uAFly-zBuSzP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omerbakler/ImageProccessOB/blob/main/OmerBakler_Notebook_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hello"
      ],
      "metadata": {
        "id": "VCUeJPzpseq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is my notebook from \"Image Proccessing\" Course, at the Hebrew University of Jerusalem Institute, Robert H. Smith Faculty of Agriculture. The course was taken in the fall semester of 2022-23 year. \n",
        "This notebook contains essential image processing tools from the very basic to machine learning. For any questions and notes please contact."
      ],
      "metadata": {
        "id": "KSqp9iFMrs5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic numpy"
      ],
      "metadata": {
        "id": "a_rTDU1QhKXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1,2,3,4,5]) # creating an array\n",
        "x.shape    #return a tuple of array dimensions\n",
        "x.size     #return number of elements in the array\n",
        "x.dtype    #return the data type of the elements in the array\n",
        "x = np.array([1, 2], dtype=np.int64)     #force a specific data type\n",
        "# Access the 0th row of a:\n",
        "x_row0 = x[0,:]"
      ],
      "metadata": {
        "id": "V6scrpawhFXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing: "
      ],
      "metadata": {
        "id": "2Sy5yqLtig7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 0th row of a:\n",
        "x_row0 = x[0,:]\n",
        "\n",
        "# Access the 0th column of a\n",
        "a_col0 = a[:,0]\n",
        "\n",
        "# arange puts all number between 2 values in the tuple\n",
        "# reshape changes the size of the array to the dimensions in the tuple\n",
        "b = np.arange(1,17).reshape((4,4))"
      ],
      "metadata": {
        "id": "zBgWgvuFifBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arithmatic operations:"
      ],
      "metadata": {
        "id": "aSeFXX1njPw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original x: \")\n",
        "print(x)\n",
        "print(\"\\nx+1\")\n",
        "print(x + 1)\n",
        "print(\"\\nx*4.5\")\n",
        "print(4.5*x)\n",
        "print(\"\\nx/2.0\")\n",
        "print(x/2.0)\n",
        "\n",
        "print(x - y)\n",
        "print('\\n', np.subtract(x, y))\n",
        "\n",
        "print(x * y)\n",
        "print('\\n',np.multiply(x, y))\n",
        "\n",
        "print(x / y)\n",
        "print('\\n',np.divide(x, y))\n",
        "\n",
        "print(np.sqrt(x))\n",
        "\n",
        "print(np.power(x,2))"
      ],
      "metadata": {
        "id": "bahhrbyhjwNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other useful functions:"
      ],
      "metadata": {
        "id": "jJK6IE0HIGmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((x,y), axis = 0)    #adding to arrays together, on the axis given.\n",
        "np.transpose(result)     #switching rows and columns\n",
        "x.dot(y)\n",
        "np.dot(x, y)     #calculating dot product\n",
        "np.matmul(x,y)   # matrix multiplication"
      ],
      "metadata": {
        "id": "S-nRmAycILie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boolean mask:"
      ],
      "metadata": {
        "id": "_xKrnW6NJL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "array = np.array(range(20)).reshape((4,5))\n",
        "output = array > 10\n",
        "print(output)\n",
        "print(array[output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNT3nFioJNzF",
        "outputId": "91e59ae4-3c13-4597-8253-ad3e3b4f5923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False False False False False]\n",
            " [False False False False False]\n",
            " [False  True  True  True  True]\n",
            " [ True  True  True  True  True]]\n",
            "[11 12 13 14 15 16 17 18 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "copy:"
      ],
      "metadata": {
        "id": "Z0xG1pMtJvf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "y = copy.deepcopy(x)\n",
        "\n",
        "y = x.copy()      #builtin numpy copy method"
      ],
      "metadata": {
        "id": "Sb4cAgRTJwsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic images tools"
      ],
      "metadata": {
        "id": "phxw8TdBKLFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprtant Libs to import:"
      ],
      "metadata": {
        "id": "BalVGvG1N-Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import skimage\n",
        "import skimage.io as io\n",
        "import cv2 "
      ],
      "metadata": {
        "id": "iTJOpLGjOBKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive and creating working folder:"
      ],
      "metadata": {
        "id": "JLnKtY9cMQV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/MyDrive/71254_2023/01_Lectures/Class02'"
      ],
      "metadata": {
        "id": "tT-Ea_aeMYYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading and displaing an image:"
      ],
      "metadata": {
        "id": "XBKxrRVrKboz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = io.imread(image_path)            #reading an image using skimage\n",
        "image = io.imread(fname=f\"{folder_path}/image.jpeg\")   #read and display image the same time\n",
        "plt.figure(figsize = (5,5))              #setting image sizes to show\n",
        "plt.imshow(image)                        #displaying the image\n",
        "plt.axis('off')                          #displaing or not the axis bars\n",
        "image = image.astype(np.float64) / 255   #Normalizing the image data type\n",
        "plt.colorbar()                           #display colorbar next to image"
      ],
      "metadata": {
        "id": "FXbecJwaKaI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excluding a specific channel from an RGB image. If it's BGR with cv2, the order should be reversed."
      ],
      "metadata": {
        "id": "ZBNuq3hKMKpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_exclusion(image, channel):\n",
        "    out = image.copy()\n",
        "    if channel == 'R':\n",
        "        out[:, :, 0] = 0\n",
        "    elif channel == 'G':\n",
        "        out[:, :, 1] = 0\n",
        "    elif channel == 'B':\n",
        "        out[:, :, 2] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "no_green = rgb_exclusion(img, 'G')\n",
        "\n",
        "b,g,r = cv2.split(image)     #splitting the image into 3 different arrays using cv2.\n",
        "\n",
        "merged_image = cv2.merge([r, g, b])   #stacked image back together, the order is rgb because it is cv2."
      ],
      "metadata": {
        "id": "-RdqRVVTMe0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert image to grayscale:"
      ],
      "metadata": {
        "id": "fxOodIGCP0As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray_image = skimage.color.rgb2gray(image)   #convert to gray scale with built in skimage function\n",
        "plt.imshow(gray_image, cmap='Greys_r')       #displaying the image, using gray color map."
      ],
      "metadata": {
        "id": "5nWc45hIP4WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying several images organized:"
      ],
      "metadata": {
        "id": "5w2GyQXZRdiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.imshow(green_channel)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(blue_channel)"
      ],
      "metadata": {
        "id": "U04YV2MKRmUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'imshow' function for displaying multiple images side by side. Input is a list of the images and list of the titles for the images."
      ],
      "metadata": {
        "id": "E20fRPumCAtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow_all(images, titles=None):\n",
        "    images = [img_as_float(img) for img in images]\n",
        "\n",
        "    if titles is None:\n",
        "        titles = [''] * len(images)\n",
        "    vmin = min(map(np.min, images))\n",
        "    vmax = max(map(np.max, images))\n",
        "    ncols = len(images)\n",
        "    height = 5\n",
        "    width = height * len(images)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=ncols,\n",
        "                             figsize=(width, height))\n",
        "    for ax, img, label in zip(axes.ravel(), images, titles):\n",
        "        ax.imshow(img, vmin=vmin, vmax=vmax, cmap = 'gray')\n",
        "        ax.set_title(label)"
      ],
      "metadata": {
        "id": "D40rtvzoCODQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multidimensional arrays and spectral indices"
      ],
      "metadata": {
        "id": "spjstl9CNEgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant libs:"
      ],
      "metadata": {
        "id": "7JMeXVbjT5yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spectral import imshow, view_cube, ndvi\n",
        "import spectral.io.envi as envi"
      ],
      "metadata": {
        "id": "Yzhip4YmT-b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files that was taken from hyperspectral camera:"
      ],
      "metadata": {
        "id": "b8Qr3aTxUDel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dark_ref = envi.open(f'{hdr_path}/DARKREF_1341.hdr', f'{hdr_path}/DARKREF_1341.raw')\n",
        "white_ref = envi.open(f'{hdr_path}/WHITEREF_1341.hdr', f'{hdr_path}/WHITEREF_1341.raw')\n",
        "data_ref = envi.open(f'{hdr_path}/1341.hdr', f'{hdr_path}/1341.raw')\n",
        "\n",
        "white_nparr = np.array(white_ref.load())    #reading into arrays\n",
        "dark_nparr = np.array(dark_ref.load())\n",
        "data_nparr = np.array(data_ref.load())"
      ],
      "metadata": {
        "id": "8yaGMWbXUJaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correcting the image with the correction formula:"
      ],
      "metadata": {
        "id": "WKhaoJk6UhuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_nparr = np.divide(\n",
        "    np.subtract(data_nparr, dark_nparr),\n",
        "    np.subtract(white_nparr, dark_nparr))\n",
        "\n",
        "imshow(corrected_nparr, (100, 100, 100))"
      ],
      "metadata": {
        "id": "uAVJ3rhyUlkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an array with the wavelength's of image:"
      ],
      "metadata": {
        "id": "LgVLlo2RU_FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bands = np.genfromtxt(f'{hdr_path}/bands.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "SYaf3XWeVMc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to show the spectral footprint of a specific pixel, given the x,y labels of the pixel and the bands:"
      ],
      "metadata": {
        "id": "Asq5_NoGVyOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pixel(img,pixel_y,pixel_x,bands):\n",
        "\n",
        "  leaf_pixel = img[\n",
        "      pixel_y:pixel_y+1,\n",
        "      pixel_x:pixel_x+1,\n",
        "      :]\n",
        "\n",
        "  leaf_pixel_squeezed = np.squeeze(leaf_pixel)\n",
        "\n",
        "  plt.plot(bands, leaf_pixel_squeezed)\n",
        "  plt.title('Spectral Footprint\\n(Pixel {},{})'.format(\n",
        "      pixel_x, pixel_y),fontsize=16)\n",
        "  plt.xlabel('Wavelength',fontsize=14)\n",
        "  plt.ylabel('Reflectance',fontsize=14)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "lxsnKQvmVvg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same, with several pixels:"
      ],
      "metadata": {
        "id": "xlZlkp7eWiK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_several_pixel(img,pixels_tuples_array,bands):\n",
        "  for pixel_xy in pixels_tuples_array:\n",
        "    leaf_pixel = img[ \n",
        "      pixel_xy[1]:pixel_xy[1]+1, #y pixel\n",
        "      pixel_xy[1]:pixel_xy[1]+1, #x pixel\n",
        "      :]\n",
        "    leaf_pixel_squeezed = np.squeeze(leaf_pixel) #squeeze\n",
        "\n",
        "    plt.plot(bands, leaf_pixel_squeezed, label =f\"x={pixel_xy[1]}, y={pixel_xy[1]}\"  )\n",
        "  plt.title('Spectral signature',fontsize=16)\n",
        "  plt.xlabel('Wavelength',fontsize=14)\n",
        "  plt.ylabel('Reflectance',fontsize=14)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "y264O_MrWnfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flip the image:"
      ],
      "metadata": {
        "id": "ALKpDZOSXLnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flipped = np.fliplr(rgb_tomato_wall)"
      ],
      "metadata": {
        "id": "XkxkSpnAXNhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index calculation"
      ],
      "metadata": {
        "id": "wJmTIAafmRMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $$NDVI = \\frac{(NIR - RED)}{(NIR + RED)}$$"
      ],
      "metadata": {
        "id": "sru8V310mkep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating image NDVI:"
      ],
      "metadata": {
        "id": "RRRnAg9hmVg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "red_channel = corrected_nparr[:,:,87] # 651.92 nm\n",
        "nir_channel = corrected_nparr[:,:,140] # 810.86 nm\n",
        "\n",
        "NDVI = (nir_channel - red_channel) / (nir_channel + red_channel) # calc manually\n",
        "vi = ndvi(corrected_nparr, 87, 140) # using the ndvi function from spectral to calc, arguments: image, red band number, nir band number"
      ],
      "metadata": {
        "id": "GOXW2LxKmX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " $$ExG = 2*GREEN - RED - BLUE$$"
      ],
      "metadata": {
        "id": "36NK8eNZmcn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# process the image for calculating the ExG index\n",
        "import cv2\n",
        "b,g,r= cv2.split(img) # split the images into channels\n",
        "np.seterr(invalid='ignore') # ignore 0/0 when dividing\n",
        "\n",
        "# function to normalize each channel\n",
        "def normalize(img):\n",
        "  return (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "\n",
        "# you can also use img_as_float - uncomment to check\n",
        "from skimage.util import img_as_float\n",
        "image = img_as_float(img)\n",
        "\n",
        "r,g,b = normalize(r), normalize(g), normalize(b) # normalizing all channels\n",
        "\n",
        "exg = 2*g-r-b # calculating the index\n",
        "exg_mean = np.nanmean(exg) # calc the mean of the image\n",
        "\n",
        "# display\n",
        "plt.imshow(exg, cmap='RdYlGn')\n",
        "plt.colorbar()\n",
        "plt.title(f'ExG Index, mean: {exg_mean:.2f}') # notice the mean printed in the title, and only 2 digits after decimal point (using a f-string). Read more about f-strings here: https://realpython.com/python-f-strings/"
      ],
      "metadata": {
        "id": "X-vD1wKKm_Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading multiple images from a folder using glob"
      ],
      "metadata": {
        "id": "IwEXWB_bn71X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading all path's into a list:"
      ],
      "metadata": {
        "id": "Qo38beUeoD-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import skimage.io as io\n",
        "\n",
        "path_list = glob.glob(f'{folder_path}/images/*.*') #Rerurns a list of file names\n",
        "print(path_list)  #Prints the list containing file names"
      ],
      "metadata": {
        "id": "X69S6fGqoH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterating over the path list and reand each image into a list of images:"
      ],
      "metadata": {
        "id": "_RpyEY_AoVry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_list=[]  # Empty list to store images from the folder.\n",
        "path = \"images/test_images/*.*\"\n",
        "for path in path_list:   #Iterate through each file in the list using for\n",
        "    print(path)     # just stop here to see all file names printed\n",
        "    img = io.imread(path)  # now, we can read each file since we have the full path\n",
        "    image_list.append(img)  #Create a list of images (not just file names but full images)"
      ],
      "metadata": {
        "id": "wVIgltMCoUmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading images"
      ],
      "metadata": {
        "id": "uWLlc48vopQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create a new folder for images if doesn't exist:"
      ],
      "metadata": {
        "id": "YS2FG1eCoxs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # import os lib\n",
        "\n",
        "def createDir(path):\n",
        "  doesExist = os.path.exists(path) # checks whether the specified path exists\n",
        "  if not doesExist:\n",
        "    os.makedirs(path) # create path, since it doesn't exist\n",
        "    print(\"The new directory was created!\")"
      ],
      "metadata": {
        "id": "7whRn5gBo3js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a specific image from a url"
      ],
      "metadata": {
        "id": "B2dlAxMLo9he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # lib to request image from web\n",
        "import shutil # lib to save the image locally\n",
        "\n",
        "def downloadImageFromURL (url='', destination_image_path=''):\n",
        "\n",
        "  # The method will take in two parameters, the url variable you created earlier, and stream: True.\n",
        "  # by adding this second argument in guarantees no interruptions will occur when the method is running.\n",
        "  res = requests.get(url, stream = True) \n",
        "\n",
        "  if res.status_code == 200:\n",
        "      with open(destination_image_path,'wb') as f:\n",
        "          shutil.copyfileobj(res.raw, f)\n",
        "      print('Image sucessfully Downloaded: ', destination_image_path)\n",
        "  else:\n",
        "      print('Image Couldn\\'t be retrieved')"
      ],
      "metadata": {
        "id": "sS9uPhm7pdXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = '/url.jpg'\n",
        "path_to_save_image = f'{images_path}/image.jpg'\n",
        "downloadImageFromURL(image_url, path_to_save_image)"
      ],
      "metadata": {
        "id": "lqXXdeuuqfRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading images directly from google images search:"
      ],
      "metadata": {
        "id": "m7tU2qACqVz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first uninstall the lib that colab has (enter 'y' to uninstall, and restart runtime)\n",
        "!pip uninstall google_images_download\n",
        "\n",
        "# then install the updated library\n",
        "!pip install git+https://github.com/Joeclinton1/google-images-download.git"
      ],
      "metadata": {
        "id": "c_9ms4wRqaau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports libs (again, since runtime was restarted)\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import skimage.io as io\n",
        "\n",
        "# import the newly installed library\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "# function to download images from Google Image Search\n",
        "def downloadImagesFromGoogle(words_to_search='', number_of_imgs=3):\n",
        "\n",
        "  response = google_images_download.googleimagesdownload() #instantiate the class\n",
        "  arguments = {\"keywords\":words_to_search,\n",
        "              \"limit\":number_of_imgs,\"print_urls\":False}\n",
        "  paths = response.download(arguments)  \n",
        "  print(paths) #print complete paths to the downloaded images"
      ],
      "metadata": {
        "id": "dGsSvOgrqmM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling our function above (images will be downloaded to the 'downloads' folder on the left)\n",
        "downloadImagesFromGoogle('corn plant,watermelon plant', 3)"
      ],
      "metadata": {
        "id": "UTeU7JbxqpVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "watermelon_paths = glob.glob('/content/downloads/watermelon plant/*.*') # get the paths of the watermelon images\n",
        "\n",
        "# read images into a list\n",
        "img_list = []\n",
        "for path in watermelon_paths:\n",
        "  img = io.imread(path)\n",
        "  img_list.append(img)"
      ],
      "metadata": {
        "id": "Bjx7vWNTq9I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the downloaded images\n",
        "fig, ax=plt.subplots(ncols=len(img_list), nrows=1, figsize=(8, 8))\n",
        "plt.tight_layout() # nicer layout\n",
        "\n",
        "# loop to plt.imshow all the images in one row\n",
        "for i in range(len(img_list)):\n",
        "  ax[i].imshow(img_list[i])\n",
        "  ax[i].set_title(f'Image {i+1}')"
      ],
      "metadata": {
        "id": "V4JQ74sjrIF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download image with one line using wget."
      ],
      "metadata": {
        "id": "UR7BNyMOeBUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_filename = 'img_example.jpg'\n",
        "\n",
        "# using wget(a command-line tool that makes it possible to download files) - first the direct link, and then optional, you can pass a filename.\n",
        "!wget https://www.dropbox.com/s/n3xts37k5sqfgok/street.jpg -O {img_filename}"
      ],
      "metadata": {
        "id": "OfflLBrxeGSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histogams"
      ],
      "metadata": {
        "id": "fv81Zu_hrvw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using opencv:\n",
        "\n",
        "\n",
        "```\n",
        "hist = cv.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])\n",
        "```\n",
        "Let's familiarize with the function and its parameters :\n",
        "\n",
        "1. **images** : it is the source image of type uint8 or float32. it should be given in square brackets, ie, \"[img]\".\n",
        "2. **channels** : it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0], [1] or [2] to calculate histogram of blue, green or red channel respectively.\n",
        "3. **mask** : mask image. To find histogram of full image, it is given as \"None\". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask.\n",
        "4. **histSize** : this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].\n",
        "5. **ranges** : this is our RANGE. Normally, it is [0,256]. (the hist and accumalte inside the ranges, enables to compute a single histogram from several sets of arrays.)\n"
      ],
      "metadata": {
        "id": "yll6W5nrr0Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One channel histogram:"
      ],
      "metadata": {
        "id": "_KBfRGrxsXeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the histogram of the red channel of image\n",
        "image = image_list[0].copy()\n",
        "hist = cv2.calcHist([image],[0],None,[256],[0,256]) # our image, first channel(0=red), no mask, 256 bins, range 0-255\n",
        "\n",
        "# plot the above computed histogram\n",
        "plt.plot(hist, color='r') # red color for the line\n",
        "plt.title('Image histogram for the red channel of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZVqZCXFPsRId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple channels histogram:"
      ],
      "metadata": {
        "id": "U8lL5o1ftPA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the histogram of all channels of our watermelon image\n",
        "img = img_list[0].copy()\n",
        "  \n",
        "# plot the above computed histogram\n",
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "for i,color in enumerate(colors):\n",
        "  hist = cv2.calcHist([img],[i],None,[256],[0,256]) # our image, channel(0/1/2), no mask, 256 bins, range 0-255\n",
        "  plt.plot(hist, color = color) # r/g/b color for each line\n",
        "plt.title('Image histogram (RGB) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VGkHZeeOsWEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram using plt, one channel:"
      ],
      "metadata": {
        "id": "HMETt50ptXbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_with_plt = plt.hist(img[:,:,1].flatten(), bins = 256, color='g')\n",
        "plt.title('Image histogram (green layer) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "igmfRCWJtauW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple channels:"
      ],
      "metadata": {
        "id": "OvDekGbjtlph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ('r','g','b') # for the line color\n",
        "\n",
        "hist = plt.hist(img.flatten(), bins = 256, color = 'orange') # plotting the total histogram\n",
        "\n",
        "#loop to plot for each channel\n",
        "for i,color in enumerate(colors):\n",
        "  hist = plt.hist(img[:, :, i].flatten(), bins = 256, color = color, alpha = 0.5)\n",
        "\n",
        "# some more details for the figure\n",
        "plt.title('Image histogram (RGB) of our watermelon image', y=1.03) # the y makes the title a bit higher\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.legend(['Total', 'Red Channel', 'Green Channel', 'Blue Channel']) # adding a legend\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8j74RPt_tkM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operaions on images"
      ],
      "metadata": {
        "id": "sLKQuQpKty_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addition"
      ],
      "metadata": {
        "id": "UQZleeYAt2ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths_list = glob.glob(f'{images_path}/addition/*.*') # paths to list\n",
        "images_list = [io.imread(path) for path in paths_list] # read all images into a list in one line\n",
        "\n",
        "for img in images_list:\n",
        "  plt.title(img.shape)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EdMZBq2Lt_Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we need to resize one of the images to fit the other:"
      ],
      "metadata": {
        "id": "t1cRh1-NuRXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resize the large one (2) to the smaller one (1)\n",
        "resized2 = cv2.resize(images_list[1], (481,209), interpolation = cv2.INTER_AREA)\n",
        "plt.imshow(resized2)\n",
        "plt.title(resized2.shape)"
      ],
      "metadata": {
        "id": "Vt9un5VLuWfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to add 2 images together:"
      ],
      "metadata": {
        "id": "P59JgjHhu0eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if images_list[0].shape == resized2.shape:\n",
        "  added_img = images_list[0] + resized2\n",
        "  plt.imshow(added_img)\n",
        "  plt.title('Two images added together')\n",
        "else:\n",
        "  print('Shapes are different, cannot add.')"
      ],
      "metadata": {
        "id": "_vrrZmn7u4z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplication:"
      ],
      "metadata": {
        "id": "38ENW6Jcv5VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For multiplication, we first need to convert the image to float:"
      ],
      "metadata": {
        "id": "Cb2_5UDJvJB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dark_img_float = img_as_float(dark_img.copy())"
      ],
      "metadata": {
        "id": "cCRd-7vCvOZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This just simple array function, can multiply by any number:"
      ],
      "metadata": {
        "id": "7kalEpyZvY1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiplyed = dark_img_float*2"
      ],
      "metadata": {
        "id": "ZRmNZLDOvYJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Division and Subtraction:"
      ],
      "metadata": {
        "id": "dhkqqK-vvsZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the size of the image we want:"
      ],
      "metadata": {
        "id": "pN2mWysevvjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objects = plt.imread(objects_path)\n",
        "no_objects = plt.imread(no_objects_path)\n",
        "size = no_objects.shape\n",
        "print(size)"
      ],
      "metadata": {
        "id": "l6oc2JT0wPoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the other image to the same size and converting to float:"
      ],
      "metadata": {
        "id": "XSQrFXPpwalG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resized_objects = cv2.resize(objects, (318,300), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "resized_objects_float = img_as_float(resized_objects)\n",
        "no_objects_float = img_as_float(no_objects)"
      ],
      "metadata": {
        "id": "Dzx5JlEdwaG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing and subtracting the images:"
      ],
      "metadata": {
        "id": "yvriKTrZwis5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "divided_image = resized_objects_float/no_objects_float\n",
        "plt.imshow(divided_image)\n",
        "plt.title(\"divided image\")\n",
        "plt.show()\n",
        "\n",
        "subtracted_image = no_objects_float - resized_objects_float\n",
        "plt.imshow(subtracted_image)\n",
        "plt.title(\"subtracted image\")"
      ],
      "metadata": {
        "id": "W77zN-dswiQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filters"
      ],
      "metadata": {
        "id": "nqCHJqrTAvle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean filter:"
      ],
      "metadata": {
        "id": "ea6nQyMAA1dN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a mean of a 3X3 square:"
      ],
      "metadata": {
        "id": "21rXoXteA65F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_kernel = np.full((3, 3), 1/9)"
      ],
      "metadata": {
        "id": "GkZQWF9wA0EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the mean kernel array to apply kernel convulation:"
      ],
      "metadata": {
        "id": "AXaOhgdzBk1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.ndimage as ndi\n",
        "mean_image = ndi.correlate(image,mean_kernel)"
      ],
      "metadata": {
        "id": "3DtPDRQ2Bhik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian filter:"
      ],
      "metadata": {
        "id": "-vrDqm6VC1tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple gaussian filter with skimage filters:"
      ],
      "metadata": {
        "id": "4zD8f6T3DATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import filters\n",
        "sigma = 1 # for the gaussian filter\n",
        "smooth = filters.gaussian(image, sigma)"
      ],
      "metadata": {
        "id": "iFWRavovDA7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gaussian filter returns a float image, regardless of input. Cast to float so the images have comparable intensity ranges."
      ],
      "metadata": {
        "id": "Ci1aDrZBDR9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import img_as_float\n",
        "float_image = img_as_float(image)"
      ],
      "metadata": {
        "id": "1HizC51wDU6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian using OpenCV:\n",
        "Agruments: image, kernel size, sigma values, border type (padding, etc.)\n",
        "cv2.BORDER_CONSTANT adds a constant color border\n"
      ],
      "metadata": {
        "id": "AEt5TAJgD1_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_using_cv2 = cv2.GaussianBlur(image, (3,3), 0, borderType=cv2.BORDER_CONSTANT) "
      ],
      "metadata": {
        "id": "7z7rw8tYD9Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsharp mask"
      ],
      "metadata": {
        "id": "Rq1A38XhE4zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "enhanced image = original + amount * (original - blurred)"
      ],
      "metadata": {
        "id": "ATIgdNsJE-DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gaussian_img = gaussian(img, sigma=2, mode='constant', cval=0.0)\n",
        "enhanced_image = img + (img - gaussian_img)*1"
      ],
      "metadata": {
        "id": "xFFzkltxFFJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vertical and horizontal kernel"
      ],
      "metadata": {
        "id": "dn9_qFgwFYhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a vertical kernel\n",
        "vertical_kernel = np.array([\n",
        "    [-1],\n",
        "    [ 0],\n",
        "    [ 1],\n",
        "])\n",
        "horizontal_kernel = (vertical_kernel.T)\n",
        "\n",
        "# convolve our above image\n",
        "gradient_vertical = ndi.correlate(pixelated.astype(float),\n",
        "                                  vertical_kernel)\n",
        "gradient_horizontal = ndi.correlate(pixelated.astype(float),\n",
        "                                  horizontal_kernel)"
      ],
      "metadata": {
        "id": "OzNKheJ0Fdyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Magnitude calculation:"
      ],
      "metadata": {
        "id": "fkIQn9IvFvMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mag = np.sqrt((gradient_horizontal)**2+(gradient_vertical)**2)"
      ],
      "metadata": {
        "id": "J9A1zA5cFued"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sobel edge detector"
      ],
      "metadata": {
        "id": "bDAaqElDGDfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_gradient = filters.sobel(img)"
      ],
      "metadata": {
        "id": "v5H7FzY9GGyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Canny edge detector"
      ],
      "metadata": {
        "id": "Xk3aBepsGmHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Process of Canny edge detection algorithm can be broken down to 5 different steps:\n",
        "\n",
        "1. Apply Gaussian filter to smooth the image in order to remove the noise\n",
        "2. Find the intensity gradients of the image\n",
        "3. Apply non-maximum suppression to get rid of spurious response to edge detection\n",
        "4. Apply double threshold to determine potential edges (supplied by the user)\n",
        "5. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges"
      ],
      "metadata": {
        "id": "9dvz5LptGupv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "canny_edge = cv2.Canny(img, 195, 200)  #Supply Thresholds 1 and 2 "
      ],
      "metadata": {
        "id": "YYlOyjg5G-pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Median filter"
      ],
      "metadata": {
        "id": "NIdGwoyoHbU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A useful filter to blur the image without losing the edges. Works like a mean filter but applies the median value instead of mean value."
      ],
      "metadata": {
        "id": "gsf7CROwHfwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Disk creates a circular structuring element, similar to a mask with specific radius\n",
        "from skimage.morphology import disk\n",
        "neighborhood = disk(radius=1)  # \"structuring element\", matrix of ones though corners are zeroes\n",
        "\n",
        "median = filters.rank.median(pixelated, neighborhood) # applying the median filter, needs 8 bit, not float."
      ],
      "metadata": {
        "id": "kpLIjVmuHy85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bilateral filter"
      ],
      "metadata": {
        "id": "Gd-4FgRoIlOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import img_as_ubyte\n",
        "\n",
        "# d - diameter of each pixel neighborhood used during filtering\n",
        "# sigmaCOlor - Sigma of grey/color space. \n",
        "# sigmaSpace - Large value means farther pixels influence each other (as long as the colors are close enough)\n",
        "bilateral_using_cv2 = cv2.bilateralFilter(img, 5, 20, 100, borderType=cv2.BORDER_CONSTANT)"
      ],
      "metadata": {
        "id": "9glhR5-wIpF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-local means (NLM) filter"
      ],
      "metadata": {
        "id": "Xw9va3sKI4XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works well for random gaussian noise but not as good for salt and pepper\n",
        "\n",
        "The non-local means algorithm replaces the value of a pixel by an average of a selection of other pixels values: small patches centered on the other pixels are compared to the patch centered on the pixel of interest, and the average is performed only for pixels that have patches close to the current patch. "
      ],
      "metadata": {
        "id": "bjYqP04OI3yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Total variation filter (TVF)"
      ],
      "metadata": {
        "id": "u97GhUA_JH04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from skimage import io, img_as_float\n",
        "from skimage.restoration import denoise_tv_chambolle\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "denoise_img = denoise_tv_chambolle(img, weight=0.1, eps=0.0002, n_iter_max=200, multichannel=False)"
      ],
      "metadata": {
        "id": "VLBWNJb5JJCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation"
      ],
      "metadata": {
        "id": "EzVnI62EJZ9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width, height = lion_img.size\n",
        "\n",
        "im_nearest = lion_img.resize((width*4 ,height), Image.NEAREST)\n",
        "\n",
        "im_bilinear = lion_img.resize((width*4 ,height), Image.BILINEAR) \n",
        "\n",
        "im_bicubic = lion_img.resize((width*4 ,height), Image.BICUBIC) \n",
        "\n",
        "imshow_all([im_nearest,im_bilinear,im_bicubic], ['nearest','bilinear','bicubic'])"
      ],
      "metadata": {
        "id": "R2SVrRqRJcXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourier transforn"
      ],
      "metadata": {
        "id": "5OMqqGAsRVTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourier transform converts an image (or any signal) from spatial (or time) domain to frequency domain. Here's an example with Python."
      ],
      "metadata": {
        "id": "KwYqcqA1RgpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Output is a 2D complex array. 1st channel real and 2nd imaginary\n",
        "#For fft in opencv input image needs to be converted to float32\n",
        "dft = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
        "\n",
        "#Rearranges a Fourier transform X by shifting the zero-frequency \n",
        "#component to the center of the array.\n",
        "#Otherwise it starts at the top left corner of the image (array)\n",
        "dft_shift = np.fft.fftshift(dft)\n",
        "\n",
        "# Circular HPF mask, center circle is 0, remaining all ones\n",
        "#Can be used for edge detection because low frequencies at center are blocked\n",
        "#and only high frequencies are allowed. Edges are high frequency components.\n",
        "#Amplifies noise.\n",
        "\n",
        "rows, cols = img.shape\n",
        "crow, ccol = int(rows / 2), int(cols / 2)\n",
        "\n",
        "mask = np.ones((rows, cols, 2), np.uint8)\n",
        "r = 80\n",
        "center = [crow, ccol]\n",
        "x, y = np.ogrid[:rows, :cols]\n",
        "mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r*r\n",
        "mask[mask_area] = 0\n",
        "\n",
        "\n",
        "# apply mask and inverse DFT\n",
        "fshift = dft_shift * mask\n",
        "\n",
        "fshift_mask_mag = 2000 * np.log(cv2.magnitude(fshift[:, :, 0], fshift[:, :, 1]))\n",
        "\n",
        "f_ishift = np.fft.ifftshift(fshift)\n",
        "img_back = cv2.idft(f_ishift)\n",
        "img_back = cv2.magnitude(img_back[:, :, 0], img_back[:, :, 1])"
      ],
      "metadata": {
        "id": "SijQg8krR60Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing holes"
      ],
      "metadata": {
        "id": "BMV2p8SYWiLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oprion 1:"
      ],
      "metadata": {
        "id": "LCDwIlLkYHkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(8,8)) # define a kernel (change size if needed)\n",
        "res = cv2.morphologyEx(img_as_ubyte(img),cv2.MORPH_OPEN,kernel) # applying the kernel to our binary (make sure it's not a boolean array)\n"
      ],
      "metadata": {
        "id": "2QwQgqJTWmRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2:"
      ],
      "metadata": {
        "id": "9lTWJB3gYI2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage as nd\n",
        "closed_mask = nd.binary_closing(mask, np.ones((5,5)))"
      ],
      "metadata": {
        "id": "ZKi2vdBsYGro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "Wx8snsEqSLXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram based segmentation"
      ],
      "metadata": {
        "id": "MvnuUU-ESdft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thresholding is a type of image segmentation, where we change the pixels of an image to make the image easier to analyze. In thresholding, we convert an image from colour or grayscale into a binary image, i.e., one that is simply black and white. Most frequently, we use thresholding as a way to select areas of interest of an image, while ignoring the parts we are not concerned with."
      ],
      "metadata": {
        "id": "rvpjsnmvSiLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we want to see the histogram of the image."
      ],
      "metadata": {
        "id": "gjXDs0GTSoHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_with_plt = plt.hist(img.flatten(), bins = 256, color='gray')"
      ],
      "metadata": {
        "id": "mZ-uYNXRSg9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the values of interest in the image. We can apply manual thresholding. Example (the value is 0.55):"
      ],
      "metadata": {
        "id": "EEtDKcD_Sw0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual thresholding by setting threshold value to numpy array\n",
        "# After thresholding we will get a binary image.\n",
        "\n",
        "mask = (img > 0.55)\n",
        "\n",
        "masked_img[mask == 0] = 0 # if mask is False, then zero the pixel in the original image\n",
        "masked_img[mask != 0] = rgb_img[mask != 0] # if mask is True, then leave the original image"
      ],
      "metadata": {
        "id": "naQMet0kS7dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do it also with openCV:"
      ],
      "metadata": {
        "id": "5JF5S85lTaJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ret1, thresh1 = cv2.threshold(img, 0.55, 1, cv2.THRESH_BINARY)\n",
        "plt.imshow(thresh1, cmap='gray')"
      ],
      "metadata": {
        "id": "YqzOKBsPTdc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otsu's method"
      ],
      "metadata": {
        "id": "Pqq2fkQgTmX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A method to apply automatic thresholding. Particularly useful for situations where the grayscale histogram of an image has two peaks that correspond to background and objects of interest."
      ],
      "metadata": {
        "id": "b4YzpIpSTo8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import img_as_ubyte\n",
        "ret2, thresh2 = cv2.threshold(img_as_ubyte(img),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "# ret2 is the value the method has picked.\n",
        "# thresh2 is the image after thresholding.\n",
        "\n",
        "plt.imshow(thresh2, cmap='gray')\n",
        "plt.title('Our mask, using cv2+otsu')"
      ],
      "metadata": {
        "id": "ThljZ-ZWT79x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also possible with skimage:"
      ],
      "metadata": {
        "id": "O86YNvGgUM-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage import filters\n",
        "\n",
        "# perform automatic thresholding\n",
        "t = skimage.filters.threshold_otsu(img)\n",
        "print(\"Found automatic threshold t=\", t, f' ({255*t})\\n')\n",
        "\n",
        "# create a binary mask with the threshold found by Otsu's method\n",
        "binary_mask = img > t\n",
        "\n",
        "# display\n",
        "plt.imshow(binary_mask, cmap='gray')"
      ],
      "metadata": {
        "id": "nuNe8aMEUQrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otsu's is also useful for multiple threshold. It is usually represent numerous segments of interest in the image."
      ],
      "metadata": {
        "id": "Tl5cuDfkUjBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.filters import threshold_multiotsu\n",
        "import cv2\n",
        "\n",
        "# Apply multi-Otsu threshold \n",
        "thresholds = threshold_multiotsu(img, classes=4)\n",
        "print(thresholds)"
      ],
      "metadata": {
        "id": "CCUOj_g0UuX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use it to classify the image into parts of interest with diffrent colors, but all the pixels within the same range will be the same."
      ],
      "metadata": {
        "id": "ot9F_KtDU1YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Digitize (segment) original image into multiple classes.\n",
        "# np.digitize assign values 0, 1, 2, 3, ... to pixels in each class.\n",
        "regions = np.digitize(img, bins=thresholds)\n",
        "plt.imshow(regions)"
      ],
      "metadata": {
        "id": "iYIFwAprU0Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose specific region:"
      ],
      "metadata": {
        "id": "-HHdV67UVDuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take a look at region 3\n",
        "plt.imshow(regions==3, cmap='gray')"
      ],
      "metadata": {
        "id": "WUktZlVzVGb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texture"
      ],
      "metadata": {
        "id": "S6fTvxXpVPHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy:\n",
        "\n",
        "In a nutshell, the entropy function gives a value that represents level of complexity in a certain section of an image. The resulting values are of course subject to the initial structuring element we chose.\n"
      ],
      "metadata": {
        "id": "yWmfO4ZCVcYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.stats import entropy\n",
        "from skimage.morphology import disk\n",
        "\n",
        "entropy_img = entropy(img, disk(7)) # play with the disk size to get better results\n",
        "plt.imshow(entropy_img)"
      ],
      "metadata": {
        "id": "g7Let1OhVhLp",
        "outputId": "0a42f62b-2218-458e-bc98-b55b351687e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-81aa53b07a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphology\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mentropy_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# play with the disk size to get better results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage.stats'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the histogram of the entropy image:"
      ],
      "metadata": {
        "id": "3euZ1kdKVUFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let us use otsu to threshold high vs low entropy regions.\n",
        "plt.hist(entropy_img.flat, bins=100, range=(0,6))  #.flat returns the flattened numpy array (1D)\n",
        "plt.show()\n",
        "thresh = threshold_otsu(entropy_img)\n",
        "print(thresh)\n",
        "\n",
        "#Now let us binarize the entropy image \n",
        "binary = entropy_img <= thresh\n",
        "\n",
        "#display\n",
        "plt.imshow(binary)"
      ],
      "metadata": {
        "id": "rvVYvSdUWHzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to get rid of holes inside the result:"
      ],
      "metadata": {
        "id": "dudg4NO-Wyng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(8,8)) # define a kernel (change size if needed)\n",
        "res = cv2.morphologyEx(img_as_ubyte(img),cv2.MORPH_OPEN,kernel) # applying the kernel to our binary (make sure it's not a boolean array)\n"
      ],
      "metadata": {
        "id": "Y_wdBH4bWyAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we invert the image:"
      ],
      "metadata": {
        "id": "EVMpAT3SW3NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_mask = cv2.bitwise_not(res)"
      ],
      "metadata": {
        "id": "qihQZyxmW919"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We multiply by the image to see only the parts we intrested in. The mask is binary so multiplying will leave only the intresting parts."
      ],
      "metadata": {
        "id": "TJAUuo35XEed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(new_mask*img, cmap='gray')"
      ],
      "metadata": {
        "id": "SxXViJnMXCQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Color Spaces"
      ],
      "metadata": {
        "id": "i9OGDcKSXWy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the different color segments to divide the image. First we need to convert from RGB to HSV."
      ],
      "metadata": {
        "id": "6KWBy7UsXb9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to hsv color space\n",
        "hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) # convert rgb to hsv\n",
        "\n",
        "mask = cv2.inRange(hsv, (100,90,90), (120,255,255)) # mask out the blue balls. range of hsv. from hsv1 to hsv2. \n",
        "# enter the link to see the hsv color space\n",
        "# https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv/48367205#48367205\n",
        "# we chose only h and s. v we made same as s\n",
        "\n",
        "#mask = cv2.inRange(hsv, (0,0,180), (180,70,255)) # White"
      ],
      "metadata": {
        "id": "JNb2Fx39XbQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually there will be holes in the spaces beacuse colors are not definite. We want to get rid of thos holes."
      ],
      "metadata": {
        "id": "V3JtZx-DX3iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage as nd\n",
        "closed_mask = nd.binary_closing(mask, np.ones((5,5)))"
      ],
      "metadata": {
        "id": "PT-u9TuQXyo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Watershed"
      ],
      "metadata": {
        "id": "RQLnzY8KuL-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watershed segmentantion is one of the most popular algorithm for image segmentation, normally used when we want to resolve one of the most difficult operations in image processing — separating similar objects in the image that are touching each other.\n",
        "\n",
        "In essence, we partition the image into two different sets:\n",
        "\n",
        "1. catchment basins — dark areas; The group of connected pixels with the same local minimum.\n",
        "2. watershed lines — Lines that divide one catchment area from another.\n",
        "\n",
        "Here is the function for watershed segmentation."
      ],
      "metadata": {
        "id": "eY_VJdwBuThU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.segmentation import watershed\n",
        "from skimage import io, img_as_ubyte\n",
        "import imutils\n",
        "\n",
        "def WSsegment(image_path,min_dist):\n",
        "  img = cv2.imread(image_path) # read image\n",
        "  \n",
        "  if img.dtype != 'uint8':\n",
        "    img_rgb = img_as_ubyte(img)\n",
        "  \n",
        "  if len(img.shape) == 1:\n",
        "    gray = img.copy()\n",
        "  else:\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # bgr to rgb\n",
        "    shifted = cv2.pyrMeanShiftFiltering(img, 21, 51)\n",
        "    gray = cv2.cvtColor(shifted, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "  \n",
        "  D = ndimage.distance_transform_edt(thresh)\n",
        "\n",
        "  localMax = peak_local_max(D, indices=False, min_distance=min_dist, labels=thresh)\n",
        "\n",
        "  markers = ndimage.label(localMax, structure=np.ones((3, 3)))[0]\n",
        "  labels = watershed(-D, markers, mask=thresh)\n",
        "\n",
        "  total_area = []\n",
        "\n",
        "  for i,label in enumerate(np.unique(labels)):\n",
        "    if label == 0:\n",
        "      continue\n",
        "    mask = np.zeros(gray.shape, dtype=\"uint8\")\n",
        "    mask[labels == label] = 255\n",
        "\n",
        "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "    c = max(cnts, key=cv2.contourArea)\n",
        "    ((x, y), r) = cv2.minEnclosingCircle(c)\n",
        "    cv2.drawContours(img_rgb, cnts, -1, (0,255,0), 1) # draw the counters\n",
        "    plt.imshow(img_rgb)"
      ],
      "metadata": {
        "id": "RGg0TurzuRkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voronoi"
      ],
      "metadata": {
        "id": "uAFly-zBuSzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import the relevant lib and change to GPU."
      ],
      "metadata": {
        "id": "2pEHQJPmzv0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyclesperanto_prototype as cle\n",
        "from skimage import exposure, img_as_ubyte\n",
        "\n",
        "gray = skimage.color.rgb2gray(image)\n",
        "\n",
        "#Normalize then scale to 255 and convert to uint8 - using skimage\n",
        "cells_8bit = img_as_ubyte(gray)"
      ],
      "metadata": {
        "id": "KstFiveCzqGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b4a2c3d2-88bf-488e-e0f6-5d26d48be9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d56de2c63098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyclesperanto_prototype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexposure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_as_ubyte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyclesperanto_prototype'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ],
      "metadata": {
        "id": "4TO5ftN313BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means clustering is an unsupervised machine learning algorithm that aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. A cluster refers to a collection of data points aggregated together because of certain similarities. For image segmentation, clusters here are different image colors/values.\n"
      ],
      "metadata": {
        "id": "QNsy9jzn17BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to flatten the array into 2 dimensions."
      ],
      "metadata": {
        "id": "Bzn_sf8y2A8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert MxNx3 image into Kx3 where K=MxN\n",
        "pixel_values  = img.reshape((-1,3))  #-1 reshape means, in this case MxN\n",
        "\n",
        "#We convert the unit8 values to float as it is a requirement of the k-means method of OpenCV\n",
        "pixel_values = np.float32(pixel_values)"
      ],
      "metadata": {
        "id": "egUIEpSB2GE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define criteria, number of clusters and apply k-means: We are going to stop the algorithm either when some number of iterations is exceeded (say 100), or if the clusters move less than some epsilon value (let's pick 0.2 here).\n",
        "\n",
        "When this criterion is satisfied, the algorithm iteration stops.\n",
        "\n",
        "1. cv.TERM_CRITERIA_EPS — stop the algorithm iteration if specified accuracy, epsilon, is reached.\n",
        "2. cv.TERM_CRITERIA_MAX_ITER — stop the algorithm after the specified number of iterations, max_iter.\n",
        "3. cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER — stop the iteration when any of the above condition is met.\n",
        "\n",
        "The below code defines the stopping criteria in OpenCV:"
      ],
      "metadata": {
        "id": "KKzan35i2Pbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define stopping criteria\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)"
      ],
      "metadata": {
        "id": "_tZdPy9o2W8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set numbers of clusters according to the objects in the."
      ],
      "metadata": {
        "id": "YHFzuBpe2aU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of clusters (K)\n",
        "k = 4\n",
        "\n",
        "attempts = 10\n",
        "_, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "# convert back to 8 bit values\n",
        "centers = np.uint8(centers)\n",
        "\n",
        "# flatten the labels array\n",
        "labels = labels.flatten()\n",
        "\n",
        "# convert all pixels to the color of the centroids\n",
        "segmented_image = centers[labels.flatten()]\n",
        "\n",
        "# reshape back to the original image dimension\n",
        "segmented_image = segmented_image.reshape(img.shape)\n",
        "# show the image\n",
        "plt.imshow(segmented_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x3HkQYoo2haU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to segment using k-means\n",
        "\n",
        "def segment_image_kmeans(img, k=3, attempts=10): \n",
        "\n",
        "    # Convert MxNx3 image into Kx3 where K=MxN\n",
        "    pixel_values  = img.reshape((-1,3))  #-1 reshape means, in this case MxN\n",
        "\n",
        "    #We convert the unit8 values to float as it is a requirement of the k-means method of OpenCV\n",
        "    pixel_values = np.float32(pixel_values)\n",
        "\n",
        "    # define stopping criteria\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "    \n",
        "    _, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n",
        "    \n",
        "    # convert back to 8 bit values\n",
        "    centers = np.uint8(centers)\n",
        "\n",
        "    # flatten the labels array\n",
        "    labels = labels.flatten()\n",
        "    \n",
        "    # convert all pixels to the color of the centroids\n",
        "    segmented_image = centers[labels.flatten()]\n",
        "    \n",
        "    # reshape back to the original image dimension\n",
        "    segmented_image = segmented_image.reshape(img.shape)\n",
        "    \n",
        "    return segmented_image, labels, centers"
      ],
      "metadata": {
        "id": "p4hkx5JyJW1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User interaction"
      ],
      "metadata": {
        "id": "L1f_MYBM7qP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forms provide an easy way to parameterize code. From a code cell, select Insert → Add form field. When you change the value in a form, the corresponding value in the code will change. "
      ],
      "metadata": {
        "id": "nNeV68ky70Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title String fields\n",
        "\n",
        "text = 'title' #@param {type:\"string\"}\n",
        "dropdown = '1st option' #@param [\"1st option\", \"2nd option\", \"3rd option\"]\n",
        "text_and_dropdown = 'value' #@param [\"1st option\", \"2nd option\", \"3rd option\"] {allow-input: true}\n",
        "\n",
        "print(text)\n",
        "print(dropdown)\n",
        "print(text_and_dropdown)"
      ],
      "metadata": {
        "id": "wJ49GJIe7u-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Date fields\n",
        "date_input = '2018-03-22' #@param {type:\"date\"}\n",
        "\n",
        "print(date_input)"
      ],
      "metadata": {
        "id": "wvwKofEV8DRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number fields\n",
        "number_input = 10.0 #@param {type:\"number\"}\n",
        "number_slider = 0 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "\n",
        "integer_input = 10 #@param {type:\"integer\"}\n",
        "integer_slider = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "print(number_input)\n",
        "print(number_slider)\n",
        "\n",
        "print(integer_input)\n",
        "print(integer_slider)"
      ],
      "metadata": {
        "id": "oy2VzQcJ8Fi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hiding code**\n",
        "\n",
        "You can change the view of the form by selecting **View → Show/hide code** or using the toolbar above the selected code cell. You can see both code and the form, just the form, or just the code."
      ],
      "metadata": {
        "id": "fywcfiVG8I5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click `Show code` in the code cell. { display-mode: \"form\" }\n",
        "\n",
        "option1 = 'A' #@param [\"A\", \"B\", \"C\"]\n",
        "print('You selected', option1)"
      ],
      "metadata": {
        "id": "0lc9hFk58N7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title UPLOAD IMAGES HERE: RUN ME.  { display-mode: \"form\" }\n",
        "from google.colab import files\n",
        "\n",
        "try:\n",
        "  uploaded = files.upload()\n",
        "except:\n",
        "  print(\"\")\n",
        "  print(\"Please use Chrome, and enable cookies!\")\n",
        "  print(\"cookie אנא היכנסו דרך דפדפן כרום במחשב, והפעילו גישה לקבצי \")\n",
        "\n",
        "# lets the user upload the file, and stores the file name in 'file_names'. The files are uploaded under /content/FILE_NAME\n",
        "file_names = uploaded.keys() "
      ],
      "metadata": {
        "id": "eYZBc4PP8Qq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aruco Markers"
      ],
      "metadata": {
        "id": "9MnuxgwIItGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Aruco detector\n",
        "parameters = cv2.aruco.DetectorParameters_create()\n",
        "aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_5X5_50)\n",
        "\n",
        "# Get Aruco marker\n",
        "corners, _, _ = cv2.aruco.detectMarkers(image, aruco_dict, parameters=parameters)\n",
        "\n",
        "# Draw polygon around the marker\n",
        "int_corners = np.int0(corners)\n",
        "cv2.polylines(image, int_corners, True, (0, 255, 0), 50)\n",
        "plt.imshow(image)\n",
        "\n",
        "# Aruco Area\n",
        "aruco_area = cv2.contourArea (corners[0])\n",
        "print('AruCo Area:',aruco_area, 'px')\n",
        "\n",
        "# Pixel to cm ratio\n",
        "pixel_cm_ratio = 5*5 / aruco_area# since the AruCo is 5*5 cm, so we devide 25 cm*cm by the number of pixels\n",
        "print('Ratio - Each pixel is',pixel_cm_ratio, 'cm*cm')"
      ],
      "metadata": {
        "id": "AcuXIfd2Ixzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Satelite"
      ],
      "metadata": {
        "id": "pEq1mtmKZ-zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "DQRzkwbbaFd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geemap\n",
        "\n",
        "import ee\n",
        "\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()\n",
        "\n",
        "import geemap\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import ee, datetime\n",
        "from pylab import *\n",
        "from matplotlib.pylab import rcParams"
      ],
      "metadata": {
        "id": "coSEFc2aZ8e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image of LANDSAT 8 RGB\n",
        "map1 = geemap.Map()\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_SR/LC08_044034_20140318')\n",
        "# Center the map and display the image.\n",
        "map1.centerObject(image, zoom=8)\n",
        "vis_params = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0.0,\n",
        "    'max': 3000,\n",
        "    'opacity': 1.0,\n",
        "    'gamma': 1.2,\n",
        "}\n",
        "map1.addLayer(image, vis_params, \"Landsat Vis\")\n",
        "map1"
      ],
      "metadata": {
        "id": "NocRQaBtaf44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get info on the image\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_SR/LC08_044034_20140318')\n",
        "props = geemap.image_props(image)\n",
        "props.getInfo()#call the data to our pc"
      ],
      "metadata": {
        "id": "X2w1QReHamCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Landsat 8 image-do NDVI\n",
        "image = ee.Image('LANDSAT/LC08/C01/T1_TOA/LC08_044034_20140318')\n",
        "# Compute the NDVI using an expression.\n",
        "NDVI = image.expression(\n",
        "    ' ((NIR - RED) / (NIR + RED))', {\n",
        "      'NIR': image.select('B5'),\n",
        "      'RED': image.select('B4'),\n",
        "}).rename('NDVI');\n",
        "palette =  ['FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718',\n",
        "               '74A901', '66A000', '529400', '3E8601', '207401', '056201',\n",
        "               '004C00', '023B01', '012E01', '011D01', '011301']\n",
        "map1.centerObject(image, 9)\n",
        "map1.addLayer(NDVI, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1"
      ],
      "metadata": {
        "id": "D62M5Lcaap5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentinel 2 image israel\n",
        "# initialize our map\n",
        "map1 = geemap.Map()\n",
        "AOI = ee.Geometry.Point(35.054342198020024, 31.35047863722117)\n",
        "se2 = ee.ImageCollection('COPERNICUS/S2').filterDate(\"2019-01-01\",\"2019-12-31\").filterBounds(AOI).first()\n",
        "#False color imagery is displayed in a combination of standard near \n",
        "# infra-red, red and green band. False color composite using near infrared,\n",
        "# red and green bands is very popular. It is most commonly used to assess plant density\n",
        "# and healht, as plants reflect near infrared and green light, while absorbing red. \n",
        "# Since they reflect more near infrared than green, plant-covered land appears deep\n",
        "#  red. Denser plant growth is darker red.\n",
        "# Cities and exposed ground are gray or tan, and water appears blue or black.\n",
        "rgb = ['B8','B4','B3']\n",
        "# set some thresholds\n",
        "rgbViz = {\"min\":0.0, \"max\":3000,\"bands\":rgb}\n",
        "# initialize our map\n",
        "     \n",
        "\n",
        "\n",
        "map1.centerObject(AOI, 7)\n",
        "map1.addLayer(se2, rgbViz, \"S2\")\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "zKf9lRg1atZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/@melqkiades/water-detection-using-ndwi-on-google-earth-engine-2919a9bf1951\n",
        "#https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/\n",
        "NDWI = se2.normalizedDifference([ 'B3','B8',]).rename('NDWI')\n",
        "palette =  ['red', 'yellow', 'green', 'cyan', 'blue']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDWI, {'min': -1, 'max': 1, 'palette':palette}, 'NDWI')\n",
        "map1"
      ],
      "metadata": {
        "id": "4KrIuiD2awrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/@melqkiades/water-detection-using-ndwi-on-google-earth-engine-2919a9bf1951\n",
        "#https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/\n",
        "NDWI = se2.normalizedDifference([ 'B3','B8',]).rename('NDWI')\n",
        "NDWIThreshold = NDWI.gte(0.0);\n",
        "NDWIMask = NDWIThreshold.updateMask(NDWIThreshold);\n",
        "palette =  ['blue']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDWIMask,  {'min': 0, 'max': 1, 'palette':palette}, 'NDWI MASK')\n",
        "map1"
      ],
      "metadata": {
        "id": "Kl3BH1wFa0O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload ndvi map of specific area."
      ],
      "metadata": {
        "id": "4N_9EV7Ma4mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NDVI = se2.expression(\n",
        "    ' ((NIR - RED) / (NIR + RED))', {\n",
        "      'NIR': se2.select('B8'),\n",
        "      'RED': se2.select('B4'),}).rename('NDVI');\n",
        "\n",
        "\n",
        "\n",
        "palette =  ['FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718',\n",
        "               '74A901', '66A000', '529400', '3E8601', '207401', '056201',\n",
        "               '004C00', '023B01', '012E01', '011D01', '011301']\n",
        "map1.centerObject(se2, 9)\n",
        "map1.addLayer(NDVI, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1"
      ],
      "metadata": {
        "id": "5ZhcLfMLa_cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get image from the past. For example, December."
      ],
      "metadata": {
        "id": "pjK5M3S8bF08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get December image, we're using the \"avg_rad\" band\n",
        "\n",
        "#se2 = ee.ImageCollection('COPERNICUS/S2').filterDate(\"2020-05-01\",\"2022-12-31\").filterBounds(geometry)#.first()\n",
        "my_boundary = ee.Geometry.Polygon([ [[35.13008776970655, 31.288173501430748],[35.13008776970655, 31.261764527107875],[35.16304675408155, 31.261764527107875],[35.16304675408155, 31.288173501430748]]] ,proj=None)\n",
        "#fc = ee.FeatureCollection([ee.Feature(ee.Geometry.Polygon([[-109.05, 41], [-109.05, 37], [-102.05, 37], [-102.05, 41])\n",
        "#viirs2017_12 = ee.ImageCollection(\"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\").filterDate(\"2017-12-01\",\"2017-12-31\").select('avg_rad').first()\n",
        "#polygon2 = ee.Geometry.Polygon([\n",
        "#  [[-122.06197,37.04748], [-122.1830,37.4748], [-122.1830,37.8032], [-122.6197,37.8032], [-122.061974,37.047484]]]);\n",
        "tls = ee.Feature(ee.FeatureCollection(my_boundary).geometry())\n",
        "\n",
        "NDVI_clip = NDVI.clip(tls)\n",
        "map1 = geemap.Map()\n",
        "map1.centerObject(NDVI_clip, zoom=8)\n",
        "map1.add_basemap('SATELLITE')\n",
        "#map1.addLayer(X.select('NDVI'), {})\n",
        "map1.addLayer(NDVI_clip.select('NDVI'), {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "#Map.addLayer(X, {'min': 0, 'max': 1, 'palette':palette}, 'NDVI')\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "Oi0Kij2nbFcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the image."
      ],
      "metadata": {
        "id": "1CJOyqbrbPNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the image, specifying scale and region.\n",
        "task = ee.batch.Export.image.toDrive(**{\n",
        "    'image': NDVI_clip,\n",
        "    'description': 'imageToDriveExample',\n",
        "    'folder':'Example_folder',\n",
        "    'scale': 10,\n",
        "    'region': my_boundary.getInfo()['coordinates']\n",
        "})\n",
        "task.start()"
      ],
      "metadata": {
        "id": "DCrkD2OHbOr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "dataset = rasterio.open('/content/imageToDriveExample.tif')\n",
        "dataset.crs\n",
        "dataset.profile\n",
        "from rasterio.plot import show\n",
        "show(dataset)\n",
        "\n",
        "band1 = dataset.read([1])\n",
        "type(band1)\n",
        "\n",
        "import geemap.colormaps as cm\n",
        "cm.list_colormaps()"
      ],
      "metadata": {
        "id": "sKyb85vKbZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More image options."
      ],
      "metadata": {
        "id": "nBdcdEgnblyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentinel 2 image israel\n",
        "AOI = ee.Geometry.Point(35.570035433516296, 33.0444322996468)\n",
        "se2 = ee.ImageCollection('COPERNICUS/S2').filterDate('2019-09-01','2020-11-01').filterBounds(AOI).first()\n",
        "#se2 = ee.ImageCollection('COPERNICUS/S2').filterDate('2019-06-01','2020-11-01').filterBounds(AOI).first()#with cloud\n",
        "\n",
        "# set some thresholds\n",
        "rgb = ['B4','B3','B2']\n",
        "# set some thresholds\n",
        "rgbViz = {\"min\":0.0, \"max\":3000,\"bands\":rgb}\n",
        "# initialize our map\n",
        "     \n",
        "\n",
        "# initialize our map\n",
        "map1 = geemap.Map()\n",
        "map1.centerObject(AOI, 7)\n",
        "map1.addLayer(se2, rgbViz, \"S2\")\n",
        "map1.addLayerControl()\n",
        "map1"
      ],
      "metadata": {
        "id": "pid1LL8Gbnwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloud masking"
      ],
      "metadata": {
        "id": "wGi57QyLcyaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#AOI = ee.Geometry.Point(-122.269, 45.701)\n",
        "AOI = ee.Geometry.Point(35.570035433516296, 33.0444322996468)\n",
        "start_date = '2019-06-01'\n",
        "end_date = '2020-09-01'\n",
        "CLOUD_FILTER = 60\n",
        "CLD_PRB_THRESH = 40\n",
        "NIR_DRK_THRESH = 0.15\n",
        "CLD_PRJ_DIST = 2\n",
        "BUFFER = 50"
      ],
      "metadata": {
        "id": "adC5OBOjc6V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build sentinel-2 collection."
      ],
      "metadata": {
        "id": "rPD7XGzQdBcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
        "    # Import and filter S2 SR.\n",
        "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date)\n",
        "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
        "\n",
        "    # Import and filter s2cloudless.\n",
        "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "        .filterBounds(aoi)\n",
        "        .filterDate(start_date, end_date))\n",
        "\n",
        "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
        "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "        'primary': s2_sr_col,\n",
        "        'secondary': s2_cloudless_col,\n",
        "        'condition': ee.Filter.equals(**{\n",
        "            'leftField': 'system:index',\n",
        "            'rightField': 'system:index'\n",
        "        })\n",
        "    }))\n",
        "#Apply the `get_s2_sr_cld_col` function to build a collection according to the parameters defined above.\n",
        "START_DATE =start_date\n",
        "END_DATE =end_date\n",
        "s2_sr_cld_col_eval = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
        "\n",
        "# Cloud components\n",
        "\n",
        "#Define a function to add the s2cloudless probability layer and derived cloud mask as bands to an S2 SR image input.\n",
        "\n",
        "def add_cloud_bands(img):\n",
        "    # Get s2cloudless image, subset the probability band.\n",
        "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "\n",
        "    # Condition s2cloudless by the probability threshold value.\n",
        "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
        "\n",
        "    # Add the cloud probability layer and cloud mask as image bands.\n",
        "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
        "#### Cloud shadow components\n",
        "\n",
        "#Define a function to add dark pixels, cloud projection, and identified \n",
        "#shadows as bands to an S2 SR image input. Note that the image input needs to be the result of the above `add_cloud_bands` \n",
        "#function because it relies on knowing which pixels are considered cloudy (`'clouds'` band)\n",
        "def add_shadow_bands(img):\n",
        "    # Identify water pixels from the SCL band.\n",
        "    not_water = img.select('SCL').neq(6)\n",
        "\n",
        "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
        "    SR_BAND_SCALE = 1e4\n",
        "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
        "\n",
        "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
        "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
        "\n",
        "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
        "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
        "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
        "        .select('distance')\n",
        "        .mask()\n",
        "        .rename('cloud_transform'))\n",
        "\n",
        "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
        "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
        "\n",
        "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
        "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
        "    \n",
        "def add_cld_shdw_mask(img):\n",
        "    # Add cloud component bands.\n",
        "    img_cloud = add_cloud_bands(img)\n",
        "\n",
        "    # Add cloud shadow component bands.\n",
        "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
        "\n",
        "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
        "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
        "\n",
        "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
        "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
        "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
        "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
        "        .rename('cloudmask'))\n",
        "\n",
        "    # Add the final cloud-shadow mask to the image.\n",
        "    return img_cloud_shadow.addBands(is_cld_shdw)"
      ],
      "metadata": {
        "id": "6a_iLfJkdEYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define cloud mask application function"
      ],
      "metadata": {
        "id": "qy3q1vMndK5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_cld_shdw_mask(img):\n",
        "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
        "    not_cld_shdw = img.select('cloudmask').Not()\n",
        "\n",
        "    # Subset reflectance bands and update their masks, return the result.\n",
        "    return img.select('B.*').updateMask(not_cld_shdw)"
      ],
      "metadata": {
        "id": "XeEPpw7PdSHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "#35.62093419073687, 33.10805948139925 crop\n",
        "#35.609261217104056, 33.101876261159404 swemp\n",
        "point = {'type':'Point', 'coordinates':[ 35.62093419073687, 33.10805948139925]};\n",
        "#point = {'type':'Point', 'coordinates':[31.35047863722117, 35.054342198020024]};\n",
        "info = s2_sr.getRegion(point,10).getInfo()\n",
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "data"
      ],
      "metadata": {
        "id": "F5tSTuO9dZ6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape image collection \n",
        "header = info[0]\n",
        "data = np.array(info[1:])\n",
        "\n",
        "iTime = header.index('time')\n",
        "time = [datetime.datetime.fromtimestamp(i/1000) for i in (data[0:,iTime].astype(int))]\n",
        "\n",
        "# List of used image bands\n",
        "#band_list = ['B4',u'B8']\n",
        "band_list = ['B4','B8']\n",
        "iBands = [header.index(b) for b in band_list]\n",
        "yData = data[0:,iBands].astype(np.float)\n",
        "\n",
        "# Calculate NDVI\n",
        "red = yData[:,0]\n",
        "nir = yData[:,1]\n",
        "ndvi = (nir - red) / (nir + red)"
      ],
      "metadata": {
        "id": "2ZRDp2MWdbfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=ndvi, index=list(range(len(ndvi))), columns=['NDVI'])\n",
        "df = df.interpolate()\n",
        "df['Date'] = pd.Series(time, index=df.index)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "a1PENFKcddY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2_sr_median = s2_sr.median()"
      ],
      "metadata": {
        "id": "0Hm9U9zRdgVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "# Load an image.\n",
        "# Center the map and display the image.\n",
        "vis_params = {\n",
        "    'bands': ['B4', 'B3', 'B2'],\n",
        "    'min': 0.0,\n",
        "    'max': 3000,\n",
        "    'opacity': 1,\n",
        "    'gamma': 1.2,\n",
        "}\n",
        "Map.addLayer(s2_sr_median, vis_params, \"Vis\")\n",
        "Map\n",
        "#its wil take time to load"
      ],
      "metadata": {
        "id": "8fXiJpfydiEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning models for image classification"
      ],
      "metadata": {
        "id": "HsDN1Va1eYi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow"
      ],
      "metadata": {
        "id": "1FWvytjweeUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libs\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "olF-x76OfCMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select an Image Classification model\n",
        "\n",
        "image_size = 224\n",
        "dynamic_size = False\n",
        "\n",
        "model_name = \"resnet_v2_152\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/classification/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/classification/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/classification/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/classification/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/classification/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/classification/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/classification/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/classification/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/classification/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/classification/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/classification/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/classification/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/classification/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/classification/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/classification/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/classification/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/classification/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/classification/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/classification/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/classification/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/classification/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/classification/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/classification/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"mobilenet_v2_100_224\": 224,\n",
        "  \"mobilenet_v2_130_224\": 224,\n",
        "  \"mobilenet_v2_140_224\": 224,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"nasnet_mobile\": 224,\n",
        "  \"pnasnet_large\": 331,\n",
        "  \"resnet_v1_50\": 224,\n",
        "  \"resnet_v1_101\": 224,\n",
        "  \"resnet_v1_152\": 224,\n",
        "  \"resnet_v2_50\": 224,\n",
        "  \"resnet_v2_101\": 224,\n",
        "  \"resnet_v2_152\": 224,\n",
        "  \"mobilenet_v3_small_100_224\": 224,\n",
        "  \"mobilenet_v3_small_075_224\": 224,\n",
        "  \"mobilenet_v3_large_100_224\": 224,\n",
        "  \"mobilenet_v3_large_075_224\": 224,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map[model_name]\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "\n",
        "max_dynamic_size = 512\n",
        "if model_name in model_image_size_map:\n",
        "  image_size = model_image_size_map[model_name]\n",
        "  dynamic_size = False\n",
        "  print(f\"Images will be converted to {image_size}x{image_size}\")\n",
        "else:\n",
        "  dynamic_size = True\n",
        "  print(f\"Images will be capped to a max size of {max_dynamic_size}x{max_dynamic_size}\")\n",
        "\n",
        "labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n",
        "\n",
        "#download labels and creates a maps\n",
        "downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n",
        "\n",
        "classes = []\n",
        "\n",
        "with open(downloaded_file) as f:\n",
        "  labels = f.readlines()\n",
        "  classes = [l.strip() for l in labels]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "28c0GnwZfNYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = hub.load(model_handle)\n",
        "\n",
        "input_shape = image.shape\n",
        "warmup_input = tf.random.uniform(input_shape, 0, 1.0)\n",
        "%time warmup_logits = classifier(warmup_input).numpy()"
      ],
      "metadata": {
        "id": "3tzZ6R94fOxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model on image\n",
        "%time probabilities = tf.nn.softmax(classifier(image)).numpy()\n",
        "\n",
        "top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
        "np_classes = np.array(classes)\n",
        "\n",
        "# Some models include an additional 'background' class in the predictions, so\n",
        "# we must account for this when reading the class labels.\n",
        "includes_background_class = probabilities.shape[1] == 1001\n",
        "\n",
        "for i, item in enumerate(top_5):\n",
        "  class_index = item if includes_background_class else item + 1\n",
        "  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n",
        "  print(line)\n",
        "\n",
        "show_image(image, '')"
      ],
      "metadata": {
        "id": "zJpkhfFIfOrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object detection"
      ],
      "metadata": {
        "id": "Y8w3lOsVfd1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's first restart our runtime, since we're going to use a different model (only for class/teaching perposes)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n",
        "# instll the dependencies\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies\n",
        "\n",
        "# load the model\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "id": "fPqbp88hfgGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "# upload an image - you can try 'citrus.jpg' from /Class10/images\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = io.imread(img_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "KKFiTFQ3fxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of links to images\n",
        "#imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(img) # accepts URL, Filename, PIL, OpenCV, Numpy..\n",
        "\n",
        "# Results\n",
        "#results.print()\n",
        "results.show()  # or .save()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)"
      ],
      "metadata": {
        "id": "h0boB2hUfzll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want, you can load an image to the model from a url, like from a image on a public google drive\n",
        "file_id = '1xDbIZ7oJkdid5c7-NtFAIGEi-zSmaUFL'\n",
        "gdrive_url = f'https://drive.google.com/u/1/uc?id={file_id}&export=download'\n",
        "\n",
        "# Inference\n",
        "results = model(gdrive_url)\n",
        "\n",
        "# Results\n",
        "results.show()"
      ],
      "metadata": {
        "id": "Sf3m0zx9f20w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if any people were detected in the image\n",
        "if 'person' in results.pandas().xyxy[0]['name'].values:\n",
        "  print('Person detected in image!')\n",
        "else:\n",
        "  print('No people detected in image.')"
      ],
      "metadata": {
        "id": "YKpVAIgef4_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Rcnn"
      ],
      "metadata": {
        "id": "5eWmOkf4f-Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "ObwUZ2cxgEyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "!dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "!ls -l /usr/lib/x86_64-linux-gnu/libcudnn.so.*\n",
        "!pip install -U -qq tensorflow==2.5.0\n",
        "exit() # Runtime restart required!"
      ],
      "metadata": {
        "id": "LgWASia3gIFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice how the TF version is now 2.5.0\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "UMumA0PMgKwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make sure we have a GPU, under device_type\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "n9uMt9XHgOOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our GPU card is probably Tesla T4 (what we get from Google)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Z3Ma-ylXgQeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataset:\n",
        "\n",
        "Use https://www.makesense.ai/ for annotation. \n",
        "1. Upload plenty of images (you can get them from Google Images) for training, and label them there. Select them with polygons exactly.\n",
        "2. When finished all images and labels, click on \"Actions\", and \"Export Annotations\".\n",
        "3. Choose \"Single file in COCO JSON format\". and then \"Export\". \n",
        "4. Call it \"train.json\".\n",
        "5. Do steps 1-4, for around 30% of images, and download a file called \"val.json\".\n",
        "6. Create a folder called \"dataset\", the contents should look like this (in the \"train\" folder you should have at least 20 images, and in your \"val\" folder you should have at least 10 images:"
      ],
      "metadata": {
        "id": "M5x0NMTigC-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch rename files script\n",
        "# use it, if you downloaded a lot of images from google, and you want to rename them from img0 to img100 for example\n",
        "\n",
        "# Python 3 code to rename multiple\n",
        "# files in a directory or folder\n",
        "\n",
        "# importing os module\n",
        "import os\n",
        "\n",
        "# Function to rename multiple files\n",
        "def main():\n",
        "\n",
        "\tfolder = \"/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/dataset/test\"\n",
        "\tfor count, filename in enumerate(os.listdir(folder)):\n",
        "\t\tdst = f\"image{str(count)}.jpg\"\n",
        "\t\tsrc =f\"{folder}/{filename}\" # foldername/filename, if .py file is outside folder\n",
        "\t\tdst =f\"{folder}/{dst}\"\n",
        "\t\t\n",
        "\t\t# rename() function will\n",
        "\t\t# rename all the files\n",
        "\t\tos.rename(src, dst)\n",
        "\n",
        "# Driver Code\n",
        "if __name__ == '__main__':\n",
        "\t\n",
        "\t# Calling main() function\n",
        "\tmain()\n"
      ],
      "metadata": {
        "id": "lxRM5CZdgZXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning the MASK-RCNN repo\n",
        "!git clone https://github.com/kairess/Mask_RCNN"
      ],
      "metadata": {
        "id": "u1bVhfbmgeI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deedeeharris/Mask_RCNN"
      ],
      "metadata": {
        "id": "2mdFZyXpgexU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libs\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "ROOT_DIR = 'Mask_RCNN'\n",
        "\n",
        "sys.path.append(ROOT_DIR) \n",
        "from mrcnn.config import Config\n",
        "import mrcnn.utils as utils\n",
        "from mrcnn import visualize\n",
        "import mrcnn.model as modellib"
      ],
      "metadata": {
        "id": "8mnnq5rPghjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pretrained model\n",
        "# This will default to sub-directories in your mask_rcnn_dir, but if you want them somewhere else, updated it here.\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "metadata": {
        "id": "fMbfPVTCgqkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration\n",
        "\n",
        "- NAME: weights, tensorboard (Save folder name)\n",
        "- IMAGES_PER_GPU: (batch size)\n",
        "- LEARNING_RATE\n",
        "- NUM_CLASSES: Number of classes to learn (requires background +1)"
      ],
      "metadata": {
        "id": "2D043cxEguTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"custom\"\n",
        "\n",
        "    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 5\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    # Number of classes (including background) - IMPORTANT TO CHANGE ACCORDING TO YOUR LABELS IN YOUR JSON\n",
        "    NUM_CLASSES = 1 + 1  # background + 1 (flowers)\n",
        "\n",
        "    # All of our training images are 1920x1012\n",
        "    IMAGE_MIN_DIM = 512\n",
        "    IMAGE_MAX_DIM = 512\n",
        "    \n",
        "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
        "    BACKBONE = 'resnet50' # resnet50\n",
        "\n",
        "    # To be honest, I haven't taken the time to figure out what these do\n",
        "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    MAX_GT_INSTANCES = 50 \n",
        "    POST_NMS_ROIS_INFERENCE = 500 \n",
        "    POST_NMS_ROIS_TRAINING = 1000 \n",
        "    \n",
        "config = TrainConfig()\n",
        "config.display()"
      ],
      "metadata": {
        "id": "dT_n1I6fg7px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the dataset"
      ],
      "metadata": {
        "id": "jusfcuEEg_Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoLikeDataset(utils.Dataset):\n",
        "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
        "        See http://cocodataset.org/#home for more information.\n",
        "    \"\"\"\n",
        "    def load_data(self, annotation_json, images_dir):\n",
        "        \"\"\" Load the coco-like dataset from json\n",
        "        Args:\n",
        "            annotation_json: The path to the coco annotations json file\n",
        "            images_dir: The directory holding the images referred to by the json file\n",
        "        \"\"\"\n",
        "        # Load json from file\n",
        "        json_file = open(annotation_json)\n",
        "        coco_json = json.load(json_file)\n",
        "        json_file.close()\n",
        "        \n",
        "        # Add the class names using the base method from utils.Dataset\n",
        "        source_name = \"coco_like\"\n",
        "        for category in coco_json['categories']:\n",
        "            class_id = category['id']\n",
        "            class_name = category['name']\n",
        "            if class_id < 1:\n",
        "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
        "                return\n",
        "            \n",
        "            self.add_class(source_name, class_id, class_name)\n",
        "        \n",
        "        # Get all annotations\n",
        "        annotations = {}\n",
        "        for annotation in coco_json['annotations']:\n",
        "            image_id = annotation['image_id']\n",
        "            if image_id not in annotations:\n",
        "                annotations[image_id] = []\n",
        "            annotations[image_id].append(annotation)\n",
        "        \n",
        "        # Get all images and add them to the dataset\n",
        "        seen_images = {}\n",
        "        for image in coco_json['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in seen_images:\n",
        "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                seen_images[image_id] = image\n",
        "                try:\n",
        "                    image_file_name = image['file_name']\n",
        "                    image_width = image['width']\n",
        "                    image_height = image['height']\n",
        "                except KeyError as key:\n",
        "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
        "                \n",
        "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
        "                image_annotations = annotations[image_id]\n",
        "                \n",
        "                # Add the image using the base method from utils.Dataset\n",
        "                self.add_image(\n",
        "                    source=source_name,\n",
        "                    image_id=image_id,\n",
        "                    path=image_path,\n",
        "                    width=image_width,\n",
        "                    height=image_height,\n",
        "                    annotations=image_annotations\n",
        "                )\n",
        "                \n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Load instance masks for the given image.\n",
        "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
        "        Args:\n",
        "            image_id: The id of the image to load masks for\n",
        "        Returns:\n",
        "            masks: A bool array of shape [height, width, instance count] with\n",
        "                one mask per instance.\n",
        "            class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        image_info = self.image_info[image_id]\n",
        "        annotations = image_info['annotations']\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
        "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
        "            for segmentation in annotation['segmentation']:\n",
        "                mask_draw.polygon(segmentation, fill=1)\n",
        "                bool_array = np.array(mask) > 0\n",
        "                instance_masks.append(bool_array)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        mask = np.dstack(instance_masks)\n",
        "        class_ids = np.array(class_ids, dtype=np.int32)\n",
        "        \n",
        "        return mask, class_ids"
      ],
      "metadata": {
        "id": "1Gfhz4CThBtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Training and Validation Datasets"
      ],
      "metadata": {
        "id": "eLVKUs2ghGYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gvitW2FyhIxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn'\n",
        "\n",
        "dataset_train = CocoLikeDataset()\n",
        "dataset_train.load_data(f'{root_folder}/dataset/train.json', f'{root_folder}/dataset/train/')\n",
        "dataset_train.prepare()\n",
        "\n",
        "dataset_val = CocoLikeDataset()\n",
        "dataset_val.load_data(f'{root_folder}/dataset/val.json', f'{root_folder}/dataset/val/')\n",
        "dataset_val.prepare()\n",
        "\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "\n",
        "print('Train', len(dataset_train.image_ids))\n",
        "print('Validation', len(dataset_val.image_ids))\n",
        "\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "metadata": {
        "id": "EHZXWQB7hKgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Training Model"
      ],
      "metadata": {
        "id": "BR-vQdI_hRQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = modellib.MaskRCNN(\n",
        "    mode=\"training\",\n",
        "    config=config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model.load_weights(\n",
        "    COCO_MODEL_PATH,\n",
        "    by_name=True,\n",
        "    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "metadata": {
        "id": "HDvuqkQohUB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Train in two stages:\n",
        "\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass layers='heads' to the train() function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass layers=\"all to train all layers."
      ],
      "metadata": {
        "id": "LwT8t5J3hXLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "start_train = time.time()\n",
        "\n",
        "model.train(\n",
        "    dataset_train,\n",
        "    dataset_val, \n",
        "    learning_rate=config.LEARNING_RATE, \n",
        "    epochs=30, \n",
        "    layers='heads')\n",
        "\n",
        "end_train = time.time()\n",
        "minutes = round((end_train - start_train) / 60, 2)\n",
        "\n",
        "print(f'Training took {minutes} minutes')"
      ],
      "metadata": {
        "id": "V9S1Op4ZhZhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save trained model to disk"
      ],
      "metadata": {
        "id": "uU71yW_bhk7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the trained model to disk\n",
        "\n",
        "import shutil\n",
        "\n",
        "original = r'/content/Mask_RCNN/logs/custom20221228T1436/mask_rcnn_custom_0030.h5'\n",
        "target = r'/content/drive/MyDrive/71254_2023/01_Lectures/Class10/mask_rcnn/mask_rcnn_custom_0030_28122022_flowers.h5'\n",
        "\n",
        "shutil.copyfile(original, target)"
      ],
      "metadata": {
        "id": "ofn4iGQFheoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare to run Inference"
      ],
      "metadata": {
        "id": "wjdmXzzKhp0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "w2U02cz2hr3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfig(TrainConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0.65 # CHANGE HERE IF YOU WANT\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "test_model = modellib.MaskRCNN(\n",
        "    mode=\"inference\", \n",
        "    config=inference_config,\n",
        "    model_dir=MODEL_DIR)\n",
        "\n",
        "model_path = test_model.find_last()\n",
        "print(model_path)\n",
        "\n",
        "test_model.load_weights(model_path, by_name=True)"
      ],
      "metadata": {
        "id": "H5bA9Fc2huHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference"
      ],
      "metadata": {
        "id": "cbG9ZfGAhxU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "\n",
        "mask_colors = [\n",
        "    (0., 0., 0.), # Background\n",
        "    (1., 0., 0.), # Red\n",
        "    (0., 1., 0.)  # Green\n",
        "]\n",
        "\n",
        "real_test_dir = f'{root_folder}/dataset/test'\n",
        "image_paths = []\n",
        "\n",
        "for filename in os.listdir(real_test_dir):\n",
        "    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
        "        image_paths.append(os.path.join(real_test_dir, filename))\n",
        "\n",
        "for image_path in image_paths:\n",
        "    img = skimage.io.imread(image_path)\n",
        "    img_arr = np.array(img)\n",
        "\n",
        "    results = test_model.detect([img_arr], verbose=1)\n",
        "    r = results[0]\n",
        "\n",
        "    colors = tuple(np.take(mask_colors, r['class_ids'], axis=0))\n",
        "\n",
        "    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
        "                                dataset_val.class_names, r['scores'], figsize=(16, 8),\n",
        "                                colors=colors)"
      ],
      "metadata": {
        "id": "-7bjp12Bhwco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run on one image"
      ],
      "metadata": {
        "id": "VbIcPV90h23F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "# upload an image \n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "img = io.imread(img_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "3ueqE2r5h4v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or download from URL using wget\n",
        "!wget https://www.seipasa.com/files/images/img_flor-de-tomate.jpg -O flower.jpg\n",
        "img = io.imread('flower.jpg')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "7OhNSBpQh7DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "\n",
        "mask_colors = [\n",
        "    (0., 0., 0.), # Background (class 0)\n",
        "    (1., 0., 0.)#, # Flower (class 1)\n",
        "    #(0., 1., 0.)  # Add here, for other classes\n",
        "]\n",
        "\n",
        "\n",
        "img_arr = np.array(img)\n",
        "\n",
        "results = test_model.detect([img_arr], verbose=1)\n",
        "r = results[0]\n",
        "\n",
        "colors = tuple(np.take(mask_colors, r['class_ids'], axis=0))\n",
        "\n",
        "visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], figsize=(16, 8),\n",
        "                            colors=colors)"
      ],
      "metadata": {
        "id": "w4jNh3_ph9yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# playing around with the results.\n",
        "# the results is a dictionary\n",
        "r.keys()"
      ],
      "metadata": {
        "id": "8kq6p77rh_4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's access the dictionary values\n",
        "masks = r.get(\"masks\")\n",
        "class_ids = r.get(\"class_ids\")\n",
        "scores = r.get(\"scores\")"
      ],
      "metadata": {
        "id": "HVaJDPQkiB3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the scores (remember, we could of changed the minimum score up top)\n",
        "scores"
      ],
      "metadata": {
        "id": "TW4KGoduiDxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying a mask for example\n",
        "plt.imshow(masks[:,:,5])"
      ],
      "metadata": {
        "id": "3BvGyridiF8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try to mask out the backgruond\n",
        "total_mask = masks[:,:,0].copy()\n",
        "for i in range(len(scores)):\n",
        "  total_mask = total_mask + masks[:,:,i]\n",
        "plt.imshow(total_mask)"
      ],
      "metadata": {
        "id": "GZCWBX_CiHkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert mask shape from (500,840) to (500,840,3) \n",
        "threeD_mask = np.stack((total_mask, total_mask, total_mask),axis=-1)"
      ],
      "metadata": {
        "id": "O5Uo8MQziJqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the flowers segmented\n",
        "plt.imshow(threeD_mask*img)"
      ],
      "metadata": {
        "id": "pgCOuMQGiLPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}